import requests
from bs4 import BeautifulSoup
import os
import urllib.parse
import getpass
import re


class BlackboardCrawler:
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.session = requests.Session()
        self.base_url = "https://bb.sustech.edu.cn"  # å—ç§‘å¤§ Blackboard åœ°å€
        self.login_url = f"{self.base_url}/webapps/login/"
        self.cas_url = "https://cas.sustech.edu.cn/cas/login"  # CAS è®¤è¯åœ°å€
        # è¯¾ç¨‹åˆ—è¡¨ AJAX æ¥å£
        self.course_list_url = f"{self.base_url}/webapps/portal/execute/tabs/tabAction"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        }

    def login(self, username, password):
        """ç™»å½• Blackboard ç³»ç»Ÿé€šè¿‡ CAS è®¤è¯"""
        # è®¿é—® Blackboard ç™»å½•é¡µè·å– CAS é‡å®šå‘
        bb_response = self.session.get(self.login_url, headers=self.headers)

        # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ° CAS ç™»å½•é¡µé¢
        cas_login_url = bb_response.url if "cas.sustech.edu.cn" in bb_response.url else f"{self.cas_url}?service={urllib.parse.quote(self.login_url)}"

        cas_response = self.session.get(cas_login_url, headers=self.headers)
        cas_soup = BeautifulSoup(cas_response.text, "xml")

        # è·å– execution token
        execution = cas_soup.find("input", {"name": "execution"})
        if not execution:
            print("âŒ æ— æ³•æ‰¾åˆ° CAS è®¤è¯çš„ execution å‚æ•°")
            return False

        execution_value = execution.get("value")

        # æäº¤ç™»å½•è¡¨å•
        cas_login_data = {
            "username": username,
            "password": password,
            "execution": execution_value,
            "_eventId": "submit",
            "geolocation": "",
            "submit": "ç™»å½•"
        }

        cas_login_response = self.session.post(
            cas_login_url,
            data=cas_login_data,
            headers=self.headers,
            allow_redirects=True
        )

        # éªŒè¯æ˜¯å¦ç™»å½•æˆåŠŸ
        if "ç™»å‡º" in cas_login_response.text or "logout" in cas_login_response.text.lower():
            print("âœ… CAS è®¤è¯æˆåŠŸï¼Œå·²ç™»å½• Blackboard!")
            return True
        else:
            print("âŒ ç™»å½•å¤±è´¥ï¼Œå¯èƒ½æ˜¯ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")
            return False

    def get_courses(self):
        """è·å–è¯¾ç¨‹åˆ—è¡¨ï¼ˆä» AJAX åŠ è½½ï¼‰"""
        print("ğŸ“¡ æ­£åœ¨è·å–è¯¾ç¨‹åˆ—è¡¨...")
        payload = {
            "action": "refreshAjaxModule",
            "modId": "_3_1",
            "tabId": "_1_1",
            "tab_tab_group_id": "_1_1"
        }
        response = self.session.post(
            self.course_list_url, headers=self.headers, data=payload)

        with open("debug_courses_page.html", "w", encoding="utf-8") as f:
            f.write(response.text)
        print("å·²ä¿å­˜é¡µé¢ HTML åˆ° debug_courses_page.html ç”¨äºè°ƒè¯•")

        if response.status_code != 200:
            print("âŒ è¯¾ç¨‹åˆ—è¡¨åŠ è½½å¤±è´¥")
            return []

        soup = BeautifulSoup(response.text, "xml")
        courses = []

        # æŸ¥æ‰¾æ‰€æœ‰è¯¾ç¨‹é“¾æ¥
        for link in soup.find_all("a", href=True):
            href = link.get("href")
            if "course_id" in href:  # åªè·å–è¯¾ç¨‹é“¾æ¥
                course_name = link.text.strip()
                course_url = f"{self.base_url}{href}"
                courses.append((course_name, course_url))

        return courses

    def get_course_pdfs(self, course_url):
        """è·å–è¯¾ç¨‹ä¸­çš„ PDF è¯¾ä»¶"""
        print(f"ğŸ“¡ æ­£åœ¨çˆ¬å– {course_url} çš„è¯¾ä»¶...")
        response = self.session.get(course_url, headers=self.headers)

        if "cas.sustech.edu.cn" in response.url:
            print("âš ï¸ ä¼šè¯å·²è¿‡æœŸï¼Œéœ€è¦é‡æ–°ç™»å½•")
            return []

        soup = BeautifulSoup(response.text, "html.parser")
        pdfs = []

        # æŸ¥æ‰¾æ‰€æœ‰ PDF æ–‡ä»¶
        for link in soup.find_all("a", href=True):
            href = link.get("href", "")
            if href.endswith(".pdf") or "/bbcswebdav/" in href:
                name = link.text.strip()
                if not name:
                    name = os.path.basename(urllib.parse.urlparse(href).path)

                pdf_url = f"{self.base_url}{href}" if href.startswith(
                    "/") else href
                pdfs.append((name, pdf_url))

        return pdfs

    def download_pdf(self, pdf_url, save_path):
        """ä¸‹è½½ PDF æ–‡ä»¶"""
        response = self.session.get(pdf_url, headers=self.headers, stream=True)
        if response.status_code == 200:
            with open(save_path, "wb") as file:
                for chunk in response.iter_content(chunk_size=1024):
                    file.write(chunk)
            print(f"âœ… {save_path} ä¸‹è½½å®Œæˆï¼")
            return True
        else:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {pdf_url}, çŠ¶æ€ç : {response.status_code}")
            return False


# è¿è¡Œçˆ¬è™«
if __name__ == "__main__":
    # username = input("è¯·è¾“å…¥ç”¨æˆ·å: ")
    # password = getpass.getpass("è¯·è¾“å…¥å¯†ç : ")
    username = '12213009'
    password = 'xwpc.769394'

    crawler = BlackboardCrawler()
    if crawler.login(username, password):
        courses = crawler.get_courses()

        if not courses:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•è¯¾ç¨‹ï¼Œå¯èƒ½æ˜¯è§£æé—®é¢˜æˆ–ä¼šè¯å·²è¿‡æœŸ")
            exit(1)

        print("\nğŸ“š ä½ å·²é€‰çš„è¯¾ç¨‹:")
        for idx, (name, url) in enumerate(courses, 1):
            print(f"{idx}. {name}")
            print(f"   ğŸ”— {url}")

        # é€‰æ‹©è¯¾ç¨‹çˆ¬å–è¯¾ä»¶
        course_index = int(input("\nè¯·è¾“å…¥è¯¾ç¨‹ç¼–å·ä»¥çˆ¬å–è¯¾ä»¶: ")) - 1
        if 0 <= course_index < len(courses):
            course_name, course_url = courses[course_index]
            print(f"\næ­£åœ¨çˆ¬å– {course_name} çš„è¯¾ä»¶...")

            pdfs = crawler.get_course_pdfs(course_url)
            if pdfs:
                download_dir = f"./downloads/{course_name}"
                os.makedirs(download_dir, exist_ok=True)

                for pdf_name, pdf_url in pdfs:
                    safe_name = re.sub(r'[\/:*?"<>|]', '_', pdf_name)  # å¤„ç†æ–‡ä»¶å
                    save_path = os.path.join(download_dir, safe_name + ".pdf")
                    crawler.download_pdf(pdf_url, save_path)

                print(f"\nâœ… {len(pdfs)} ä¸ªæ–‡ä»¶å·²ä¸‹è½½åˆ° {download_dir}")
            else:
                print("âš ï¸ è¯¥è¯¾ç¨‹æ²¡æœ‰æ‰¾åˆ°å¯ä¸‹è½½çš„ PDF æ–‡ä»¶ï¼")
        else:
            print("âŒ æ— æ•ˆçš„è¯¾ç¨‹ç¼–å·ï¼")
