from io import text_encoding
import xml.etree.ElementTree as ET
import requests
from bs4 import BeautifulSoup
import os
import urllib.parse
import re
import yaml
import json
import shutil
from tqdm import tqdm


class BlackboardCrawler:
    def __init__(self):
        """initialize cravler"""

        # personal username & password
        self.user_info_path = "./login.yaml"

        # bb-vault bath path
        self.base_path = "./bb-vault/"

        self.session = requests.Session()
        self.base_url = "https://bb.sustech.edu.cn"
        self.login_url = f"{self.base_url}/webapps/login/"
        self.cas_url = "https://cas.sustech.edu.cn/cas/login"

        # è¯¾ç¨‹åˆ—è¡¨ AJAX æ¥å£
        self.course_list_url = f"{self.base_url}/webapps/portal/execute/tabs/tabAction"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        }

        self.DEBUG = False

    def login(self):
        """ç™»å½• Blackboard ç³»ç»Ÿé€šè¿‡ CAS è®¤è¯"""
        with open(self.user_info_path, "r", encoding="utf-8") as file:
            info = yaml.safe_load(file)
            username = info["username"]
            password = info["password"]

        # è®¿é—® Blackboard ç™»å½•é¡µè·å– CAS é‡å®šå‘
        bb_response = self.session.get(self.login_url, headers=self.headers)

        # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ° CAS ç™»å½•é¡µé¢
        cas_login_url = (
            bb_response.url
            if "cas.sustech.edu.cn" in bb_response.url
            else f"{self.cas_url}?service={urllib.parse.quote(self.login_url)}"
        )

        cas_response = self.session.get(cas_login_url, headers=self.headers)
        cas_soup = BeautifulSoup(cas_response.text, "xml")

        # è·å– execution token
        execution = cas_soup.find("input", {"name": "execution"})
        if not execution:
            print("âŒ æ— æ³•æ‰¾åˆ° CAS è®¤è¯çš„ execution å‚æ•°")
            return False

        execution_value = execution.get("value")

        # print(execution_value)

        # æäº¤ç™»å½•è¡¨å•
        cas_login_data = {
            "username": username,
            "password": password,
            "execution": execution_value,
            "_eventId": "submit"
            # "geolocation": "",
            # "submit": "ç™»å½•"
        }

        cas_login_response = self.session.post(
            cas_login_url, data=cas_login_data, headers=self.headers, allow_redirects=True
        )

        # print(cas_login_response.text)

        # éªŒè¯æ˜¯å¦ç™»å½•æˆåŠŸ
        if "ç™»å‡º" in cas_login_response.text or "logout" in cas_login_response.text.lower():
            print("CAS è®¤è¯æˆåŠŸï¼Œå·²ç™»å½• Blackboard!")
            return True
        else:
            print("ç™»å½•å¤±è´¥ï¼Œå¯èƒ½æ˜¯ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")
            return False

    def print_courses_info(self, courses, which_term=None, annoucement=False):
        for term, course_list in courses.items():
            if which_term is not None and term != which_term:
                continue
            print(f"\nğŸ“š {term}:")
            for course in course_list:
                print(f"  - {course['name']}\n    + {course['url']}")
                if annoucement == True and course["announcement"]:
                    print("    ğŸ“¢ å…¬anoucnement")
                    for ann_text, ann_url in course["announcement"]:
                        print(f"      - {ann_text}: {ann_url}")

    def parse_vault(self):
        """ä»bbä¸»é¡µè·å–è¯¾ç¨‹åˆ—è¡¨ï¼ˆä» AJAX åŠ è½½ï¼‰"""

        print("ğŸ“¡ æ­£åœ¨è·å–è¯¾ç¨‹åˆ—è¡¨...")
        payload = {"action": "refreshAjaxModule", "modId": "_3_1", "tabId": "_1_1", "tab_tab_group_id": "_1_1"}
        response = self.session.post(self.course_list_url, headers=self.headers, data=payload)

        if response.status_code != 200:
            print("âŒ è¯¾ç¨‹åˆ—è¡¨åŠ è½½å¤±è´¥")
            return []

        xml_data = response.text

        # **è§£æ XMLï¼Œæå– CDATA å†…çš„ HTML**
        try:
            root = ET.fromstring(xml_data)
            html_content = root.text  # ç›´æ¥å– root.text å¯èƒ½ä¸ºç©º
            if not html_content:
                print("âš ï¸ æå–çš„ HTML ä¸ºç©ºï¼Œå¯èƒ½è§£æé”™è¯¯")
                return []

            # **ä½¿ç”¨ BeautifulSoup è§£æ HTML**
            soup = BeautifulSoup(html_content, "html.parser")

            # å­˜å‚¨è¯¾ç¨‹ä¿¡æ¯
            courses = {}

            # **éå†æ‰€æœ‰å­¦æœŸ**
            for term in soup.find_all("h3", class_="termHeading-coursefakeclass"):
                term_name = term.get_text(strip=True)  # è·å–å­¦æœŸåç§°
                match = re.search(r"ï¼ˆ(Spring|Fall|Summer|Winter) (\d{4})ï¼‰", term_name)
                if match:
                    season = match.group(1).lower()  # è½¬å°å†™
                    year = match.group(2)[-2:]  # è·å–å¹´ä»½åä¸¤ä½
                    term_name = f"{year}{season}"
                else:
                    term_name = "unknown"

                courses[term_name] = []

                # ğŸ”¹ è·å–å­¦æœŸå¯¹åº”çš„è¯¾ç¨‹åˆ—è¡¨ `<div>`
                a_tag = term.find("a", id=True)
                if a_tag:
                    term_id_match = re.search(r"termCourses__\d+_\d+", a_tag["id"])
                    if term_id_match:
                        term_id = "_3_1" + term_id_match.group()  # ç¡®ä¿ ID ç»“æ„å®Œæ•´
                        course_list_div = soup.find("div", id=term_id)

                        if course_list_div:
                            # éå†è¯¥å­¦æœŸçš„æ‰€æœ‰è¯¾ç¨‹
                            for course_li in course_list_div.find_all("li"):
                                course_link = course_li.find("a", href=True)

                                # ğŸ›‘ **è·³è¿‡å…¬å‘Šçš„ `<a>`ï¼Œåªå¤„ç†è¯¾ç¨‹**
                                if not course_link or "announcement" in course_link["href"]:
                                    continue  # å¦‚æœæ˜¯å…¬å‘Šï¼Œè·³è¿‡

                                    # âœ… è¯¾ç¨‹ä¿¡æ¯
                                course_name = course_link.get_text(strip=True)
                                course_url = course_link["href"].strip()
                                full_course_url = f"https://bb.sustech.edu.cn{course_url}"

                                # **æŸ¥æ‰¾å…¬å‘Šä¿¡æ¯**
                                announcements = {}
                                course_data_block = course_li.find("div", class_="courseDataBlock")
                                if course_data_block:
                                    # **ç§»é™¤ "å…¬å‘Š: " æ ‡ç­¾**
                                    span_label = course_data_block.find("span", class_="dataBlockLabel")
                                    if span_label:
                                        span_label.extract()  # åˆ é™¤ "å…¬å‘Š: " è¿™ä¸ªæ ‡ç­¾

                                    # **éå†å…¬å‘Šä¿¡æ¯**
                                    for ann in course_data_block.find_all("a", href=True):
                                        announcements["content"] = ann.get_text(strip=True)
                                        announcements["url"] = f"https://bb.sustech.edu.cn{ann['href'].strip()}"

                                # âœ… **å­˜å‚¨è¯¾ç¨‹æ•°æ®**
                                courses[term_name].append(
                                    {
                                        "name": course_name,
                                        "url": full_course_url,
                                        "announcement": announcements,  # è¿™é‡Œä¸å†åŒ…å«é”™è¯¯çš„è¯¾ç¨‹
                                    }
                                )

            if self.DEBUG:
                # **ä¿å­˜ HTML ä»¥ä¾¿è°ƒè¯•**
                with open("cache/debug-main-page.html", "w", encoding="utf-8") as f:
                    f.write(response.text)
                print("âœ… å·²ä¿å­˜é¡µé¢ HTML åˆ° cache/debug-main-page.html ç”¨äºè°ƒè¯•")

                with open("cache/courses.json", "w", encoding="utf-8") as f:
                    json.dump(courses, f, ensure_ascii=False, indent=4)
                print("âœ… è¯¾ç¨‹æ•°æ®å·²æˆåŠŸä¿å­˜åˆ° cache/courses.jsonï¼")

            return courses

        except ET.ParseError as e:
            print(f"âŒ XML è§£æé”™è¯¯: {e}")
            return None

    def parse_course(self, url):
        """ä»è¯¾ç¨‹ä¸»é¡µæŠ“å–é¡µé¢ HTML å¹¶æå–ä¾§è¾¹æ çš„sessioné“¾æ¥"""

        try:
            # å‘é€è¯·æ±‚å¹¶è·Ÿéšé‡å®šå‘
            response = self.session.get(url, allow_redirects=True)
            response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ

            final_url = response.url  # è·å–æœ€ç»ˆçš„ URL
            print(f"ğŸ”€ å·²é‡å®šå‘åˆ°: {final_url}")

            # è§£æ HTML
            soup = BeautifulSoup(response.text, "html.parser")

            # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
            os.makedirs("cache", exist_ok=True)

            # ä¿å­˜å®Œæ•´çš„ HTML é¡µé¢
            html_path = "cache/debug-site-page.html"
            with open(html_path, "w", encoding="utf-8") as file:
                file.write(response.text)
            print(f"âœ… é¡µé¢å·²ä¿å­˜åˆ° {html_path}")

            # æå–ä¾§è¾¹æ ç»“æ„
            sidebar_structure = self.extract_sidebar_links(soup)

            if self.DEBUG:
                # ä¿å­˜è§£æåçš„ JSON
                json_path = "cache/sidebar_links.json"
                with open(json_path, "w", encoding="utf-8") as json_file:
                    json.dump(sidebar_structure, json_file, indent=4, ensure_ascii=False)
                print(f"âœ… ä¾§è¾¹æ é“¾æ¥å·²è§£æå¹¶ä¿å­˜åˆ° {json_path}")

            return sidebar_structure

        except requests.exceptions.RequestException as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            return None

    def extract_sidebar_links(self, soup):
        """session HTML -> page url & name"""

        sidebar_menu = {}

        # æ‰¾åˆ°è¯¾ç¨‹èœå• ul æ ‡ç­¾
        menu_ul = soup.find("ul", id="courseMenuPalette_contents")
        if not menu_ul:
            print("âŒ æœªæ‰¾åˆ°è¯¾ç¨‹èœå•")
            return {}

        # è¯¾ç¨‹ IDï¼ˆç”¨äºæ„é€ æ­£ç¡®çš„ Announcements é“¾æ¥ï¼‰
        course_id_match = re.search(r"course_id=(_\d+_\d+)", str(soup))
        course_id = course_id_match.group(1) if course_id_match else None

        current_category = None
        for li in menu_ul.find_all("li", recursive=False):
            # å¤„ç†åˆ†ç±»æ ‡é¢˜ï¼ˆ<h3>ï¼‰
            category_tag = li.find("h3")
            if category_tag:
                current_category = category_tag.get_text(strip=True)
                sidebar_menu[current_category] = []
                continue  # è·³è¿‡å½“å‰ <li> çš„åç»­è§£æ

            # å¤„ç†è¯¾ç¨‹å†…å®¹é“¾æ¥
            link_tag = li.find("a", href=True)
            if link_tag:
                link_text = link_tag.get_text(strip=True)
                link_url = f"https://bb.sustech.edu.cn{link_tag['href']}"

                # ç‰¹æ®Šå¤„ç† Announcementsï¼ˆæ›¿æ¢ URLï¼‰
                if "Announcements" in link_text and course_id:
                    link_url = f"https://bb.sustech.edu.cn/webapps/blackboard/execute/announcement?method=search&context=course_entry&course_id={course_id}&handle=announcements_entry&mode=view"

                # æ·»åŠ åˆ°å½“å‰åˆ†ç±»
                if current_category:
                    sidebar_menu[current_category].append({"title": link_text, "url": link_url})
                else:
                    # å¦‚æœæ²¡æœ‰åˆ†ç±»ï¼Œç›´æ¥å­˜å…¥æ ¹ç»“æ„
                    sidebar_menu[link_text] = link_url

        return sidebar_menu

    def parse_page(self, url):
        """ä»pageä¸­æå–entriesçš„nameå’Œå†…å®¹"""

        try:
            # å‘é€è¯·æ±‚å¹¶è·Ÿéšé‡å®šå‘
            response = self.session.get(url, allow_redirects=True)
            response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ

            final_url = response.url  # è·å–æœ€ç»ˆçš„ URL
            print(f"ğŸ”€ å·²é‡å®šå‘åˆ°: {final_url}")

            # è§£æ HTML
            soup = BeautifulSoup(response.text, "html.parser")

            page = self.extract_file_structure(soup)

            if self.DEBUG:
                # ** ä¿å­˜ JSON**
                output_path = "cache/extracted_files.json"
                with open(output_path, "w", encoding="utf-8") as json_file:
                    json.dump(file_structure, json_file, ensure_ascii=False, indent=4)
                print(f"âœ… æå–çš„æ–‡ä»¶ç»“æ„å·²ä¿å­˜åˆ° {output_path}")

                # ä¿å­˜å®Œæ•´çš„ HTML é¡µé¢
                html_path = "cache/debug-page-page.html"
                with open(html_path, "w", encoding="utf-8") as file:
                    file.write(response.text)
                print(f"âœ… é¡µé¢å·²ä¿å­˜åˆ° {html_path}")

            return page

        except requests.exceptions.RequestException as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            return None

    def extract_file_structure(self, soup):
        """è§£æ Blackboard é¡µé¢ï¼Œæå–æ–‡ä»¶å’Œæ–‡æœ¬ç»“æ„"""
        if soup is None:
            print("âŒ è§£æå¤±è´¥ï¼Œæ— æ³•æå–æ–‡ä»¶ç»“æ„")
            return {}

        file_structure = {}

        # éå†æ‰€æœ‰çš„å†…å®¹åŒºåŸŸ
        for item in soup.find_all("li", class_="clearfix liItem read"):
            # è·å–å‘¨æ¬¡æ ‡é¢˜
            week_title_tag = item.find("h3")
            if not week_title_tag:
                continue

            week_title = week_title_tag.get_text(strip=True)
            content = ""

            # **1ï¸âƒ£ æå–æ–‡æœ¬ä¿¡æ¯**
            details_div = item.find("div", class_="details")
            if details_div:
                content = details_div.get_text("\n", strip=True)  # æå–çº¯æ–‡æœ¬ï¼Œä¿æŒæ¢è¡Œ

            # **2ï¸âƒ£ è·å–æ–‡ä»¶åˆ—è¡¨**
            files = []
            for file_li in item.find_all("li"):
                file_link = file_li.find("a", href=True)
                if file_link:
                    file_name = file_link.get_text(strip=True)
                    file_url = file_link["href"].strip()

                    # **è¿‡æ»¤æ‰æ— æ•ˆ URL**
                    if file_url.startswith("#") or "close" in file_url:
                        continue

                    # **è½¬æ¢ç›¸å¯¹ URL**
                    if not file_url.startswith("http"):
                        file_url = f"{self.base_url}{file_url}"

                    # **ç¡®ä¿æ–‡ä»¶åä¸ä¸ºç©º**
                    if file_name:
                        files.append({"name": file_name, "url": file_url})

            # **3ï¸âƒ£ ç»„ç»‡æ•°æ®ç»“æ„**
            file_structure[week_title] = {"text": content, "files": files}

        return file_structure

    def download_file(self, url, save_path):
        """ä¸‹è½½æ–‡ä»¶ï¼Œå¸¦é”™è¯¯å¤„ç†å’Œè·³è¿‡å¤±è´¥é¡¹"""

        # **1ï¸âƒ£ ç¡®ä¿æ–‡ä»¶åå®‰å…¨**
        safe_filename = os.path.basename(save_path).replace(" ", "_")
        save_path = os.path.join(os.path.dirname(save_path), safe_filename)

        try:
            # **2ï¸âƒ£ å°è¯•æ­£å¸¸ä¸‹è½½**
            response = self.session.get(url, stream=True, timeout=10, verify=True)
            response.raise_for_status()
        except requests.exceptions.SSLError:
            print(f"âš ï¸ SSL å¤±è´¥ï¼Œå°è¯•é™çº§ SSL è¿æ¥: {url}")
            try:
                response = self.session.get(url, stream=True, timeout=10, verify=False)  # ä¸éªŒè¯ SSLï¼ˆä»…ç”¨äºè°ƒè¯•ï¼‰
            except requests.exceptions.RequestException as e:
                print(f"âŒ SSL é™çº§ä»å¤±è´¥ï¼Œè·³è¿‡æ–‡ä»¶: {url} - {e}")
                return False  # è·³è¿‡è¯¥æ–‡ä»¶

        except requests.exceptions.RequestException as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡æ–‡ä»¶: {url} - {e}")
            return False  # è·³è¿‡è¯¥æ–‡ä»¶

        # **3ï¸âƒ£ è·å–æ–‡ä»¶å¤§å°**
        total_size = int(response.headers.get("content-length", 0))

        # **4ï¸âƒ£ é€å—å†™å…¥æ–‡ä»¶ï¼Œå¹¶æ˜¾ç¤ºè¿›åº¦**
        try:
            with open(save_path, "wb") as file, tqdm(
                desc=f"â¬‡ï¸ {safe_filename}",
                total=total_size,
                unit="B",
                unit_scale=True,
                unit_divisor=1024,
            ) as bar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        file.write(chunk)
                        bar.update(len(chunk))
        except Exception as e:
            print(f"âŒ æ–‡ä»¶å†™å…¥å¤±è´¥ï¼Œè·³è¿‡æ–‡ä»¶: {save_path} - {e}")
            return False

        print(f"âœ… ä¸‹è½½å®Œæˆ: {save_path}")
        return True  # æ–‡ä»¶ä¸‹è½½æˆåŠŸ

    def crawl(self, terms):
        """çˆ¬å–æŒ‡å®šå­¦æœŸçš„è¯¾ç¨‹"""

        # clear cache
        if os.path.exists(self.base_path):
            shutil.rmtree(self.base_path)
        os.makedirs(self.base_path, exist_ok=True)
        print(f"ğŸ—‘ï¸  æ¸…ç©ºæ–‡ä»¶å¤¹: {self.base_path}")

        vault = self.parse_vault()

        for term in terms:
            term_path = os.path.join(self.base_path, term)
            os.makedirs(term_path, exist_ok=True)

            courses = vault[term]

            for course in courses:
                course_name = course["name"].replace(" ", "_")
                course_url = course["url"]

                course_path = os.path.join(term_path, course_name)
                os.makedirs(course_path, exist_ok=True)

                sessions = self.parse_course(course_url)

                for session_name, pages in sessions.items():
                    session_path = os.path.join(course_path, session_name.replace(" ", "_"))
                    os.makedirs(session_path, exist_ok=True)

                    for page in pages:
                        page_name = page["title"].replace(" ", "_")
                        page_url = page["url"]

                        page_path = os.path.join(session_path, page_name)
                        os.makedirs(page_path, exist_ok=True)

                        entries = self.parse_page(page_url)

                        if not entries:
                            continue

                        # **ä¸‹è½½é™„ä»¶**
                        for entry_name, entry in entries.items():
                            entry_path = os.path.join(page_path, entry_name)
                            os.makedirs(entry_path, exist_ok=True)

                            text = entry["text"]

                            # **å­˜å‚¨æ–‡æœ¬**
                            if text != "":
                                text_file_path = os.path.join(entry_path, "text.txt")
                                with open(text_file_path, "w", encoding="utf-8") as text_file:
                                    text_file.write(text)
                                print(f"ğŸ“„ æ–‡å­—å†…å®¹å·²ä¿å­˜: {text_file_path}")

                            for file in entry.get("files", []):
                                file_name = file["name"].replace(" ", "_")
                                file_url = file["url"]
                                file_path = os.path.join(entry_path, file_name)
                                self.download_file(file_url, file_path)

            print(f"ğŸ“¥ {term}çš„è¯¾ç¨‹èµ„æ–™çˆ¬å–å®Œæ¯•ï¼")


if __name__ == "__main__":

    terms = ["25spring"]

    crawler = BlackboardCrawler()
    if crawler.login():

        # debug logic

        # courses = crawler.update_sites()
        # crawler.print_courses_info(courses, which_term=term, annoucement=False)
        #
        # url = courses[term][2]['url']
        # print(url)
        # pages = crawler.parse_course(url)
        #
        # crawler.parse_page(pages['Course Materials'][1]['url'])

        # start to crawl
        crawler.crawl(terms)
